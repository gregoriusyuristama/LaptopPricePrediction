# -*- coding: utf-8 -*-
"""Proyek_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TY7s4_ndfkrM0oK_-rWh0f8aDK03Roud

Install API Kaggle untuk download dataset
"""

! pip install -q kaggle

from google.colab import files

files.upload()

! mkdir ~/.kaggle

! cp kaggle.json ~/.kaggle/

! chmod 600 ~/.kaggle/kaggle.json

! kaggle datasets download -d muhammetvarl/laptop-price

! unzip /content/laptop-price.zip

"""Import Library yang diperlukan"""

import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import  OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.model_selection import GridSearchCV, cross_val_score

"""Helper Function"""

# Fungsi bantuan untuk mengubah dari TB ke GB
def converter(x):
    try:
        return pd.eval(x)
    except:
        return x

# Fungsi bantuan untuk mengevaluasi grid search
def evaluate(model, test_features, test_labels):
    mse = mean_squared_error(y_true=test_labels, y_pred=model.predict(test_features))/1e3
    print('Model Performance')
    print('MSE = ',mse)
    
    return mse

"""# **Tahapan Loading data dan Data Cleaning**





"""

df = pd.read_csv('/content/laptop_price.csv', encoding='latin-1')
df.head()

"""Drop column laptop_ID karena tidak diperlukan"""

df.drop(columns='laptop_ID', inplace=True)

"""Mengecek Missing Value"""

df.isnull().sum().sort_values(ascending=False)

"""Mengecek tipe data"""

df.info()

df['Weight']

# kolom weight bertipe object sehingga perlu diubah ke float
df['weight_float']=df.Weight.replace('kg','',regex=True).astype(float)
df.drop(['Weight'], axis=1,inplace=True)
df['weight_float']

df['Ram']

# Kolom ram bertipe object sehingga perlu diubah ke float
df['ram_int']=df.Ram.replace('GB','',regex=True).astype(int)
df.drop(['Ram'], axis=1,inplace=True)
df['ram_int']

"""Kolom ScreenResolution memiliki beberapa informasi yang dapat dipisah"""

df['ScreenResolution'].value_counts()

"""Memisahkan kolom ScreenResolution menjadi resolution_width dan resolution_height"""

df['resolution_width']=df['ScreenResolution'].str.split().str[-1].str.split("x").str[0].astype(int)
df['resolution_height']=df['ScreenResolution'].str.split().str[-1].str.split("x").str[1].astype(int)
df.drop(['ScreenResolution'],axis=1,inplace=True)
df[['resolution_width','resolution_height']]

"""Kolom Memory memiliki beberapa informasi yang dapat dipisah"""

df['Memory'].value_counts()

"""Membagi kolom Memory menjadi 4 bagian, yaitu memory_1_size, memory_1_type, memory_2_size, memory_2_type"""

df['memory_count']=df['Memory'].str.split('+').str.len()
df['memory_1'] = df['Memory'].str.split('+').str[0]
df['memory_2'] = np.where(df['memory_count'] == 2, df['Memory'].str.split('+').str[1], 0)

df['memory_1_size'] = df['memory_1'].str.split().str[0]
df['memory_1_size'] = df['memory_1_size'].replace({'GB':'','TB':'*1024'},regex=True).map(converter)
df['memory_1_type'] = df['memory_1'].str.split(r'GB|TB').str[-1].str.strip()

df['memory_2_size'] = np.where(df['memory_count'] == 2, df['memory_2'].str.split().str[0],0)
df['memory_2_size'] = df['memory_2_size'].replace({'GB':'','TB':'*1024'},regex=True).map(converter)
df['memory_2_type'] = np.where(df['memory_count'] == 2,df['memory_2'].str.split(r'GB|TB').str[-1].str.strip(),"None")

df.drop(['Memory'], axis=1,inplace=True)

df.drop(['memory_1', 'memory_2'], axis=1,inplace=True)

df[['memory_1_size','memory_1_type','memory_2_size', 'memory_2_type']]

"""Membagi kolom Cpu menjadi 3 yaitu : cpu_brand, cpu_clock, dan cpu_series"""

df['Cpu'].value_counts()

df['cpu_brand'] = df['Cpu'].str.split().str[0]
df['cpu_clock'] = df['Cpu'].str.split().str[-1].replace('GHz','',regex=True).astype(float)
df['cpu_series'] = np.where(df['Cpu'].replace(r"Dual Core|Quad Core|-Series", 
                                              '', 
                                              regex=True
                                              ).str.split().str[-3] == 'Core', df['Cpu']
                            .replace(r"Dual Core|Quad Core|-Series", 
                                     '', 
                                     regex=True
                                     ).str.split().str[-2], df['Cpu']
                            .replace(r"Dual Core|Quad Core|-Series|E3-1505M|E3-1535M", 
                                     '', 
                                     regex=True).str.split().str[-3])
df.drop(['Cpu'], axis=1,inplace=True)

df[['cpu_clock', 'cpu_brand', 'cpu_series']]

df.describe()

"""# **Data analysis**

Mengatasi Outlier
"""

sns.boxplot(x=df['Inches'])

sns.boxplot(x=df['weight_float'])

df.shape

"""Dari boxplot diatas dapat dilihat terdapat outlier, sehingga outlier tersebut perlu dihapus agar mendapatkan hasil yang lebih baik


"""

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR=Q3-Q1
df=df[~((df<(Q1-1.5*IQR))|(df>(Q3+1.5*IQR))).any(axis=1)]

df.shape

"""##**Univariate Analysis**"""

#convert all categorical columns to one hot encodding
cat_columns = sorted([col for col in df.columns if df[col].dtype=="O"])
cat_columns

num_features = df.select_dtypes(include=['float','int']).columns.to_list()
num_features

"""Mencari insight pada kategori Company"""

feature = cat_columns[0]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df_view = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_view)
count.plot(kind='bar', title=feature);

"""Pada barplot company diatas dapat dilihat bahwa pada dataset merk HP merupakan merk yang paling banyak dijual"""

feature = cat_columns[2]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df_view = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_view)
count.plot(kind='bar', title=feature);

"""Pada berplot OpSys diatas dapat dilihat bahwa sistem operasi cukup bermacam-macam dan yang paling banyak adalah laptop dengan sistem operasi windows 10"""

feature = cat_columns[5]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df_view = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_view)
count.plot(kind='bar', title=feature);

"""Pada barplot diatas dapat disimpulkan bahwa laptop dengan prosesor intel lebih banyak dijual dibanding AMD


"""

feature = cat_columns[6]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df_view = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_view)
count.plot(kind='bar', title=feature);

"""Dapat disimpulkan berdasarkan barplot diatas seri CPU yang paling banyak dijual adalah i5


"""

feature = cat_columns[7]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df_view = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df_view)
count.plot(kind='bar', title=feature);

"""Dapat dilihat berdasarkan barplot diatas, kebanyakan laptop yang dijual sudah menggunakan SSD


"""

df.hist(bins=50, figsize=(20,15))
plt.show()

"""Karena pada histogram memory_count dan memory_2_size hanya memiliki 1 data saja maka kolom tersebut didrop"""

df.drop(['memory_count', 'memory_2_size'], axis=1,inplace=True)

"""## **Multivariate** Analysis"""

for col in cat_columns:
  sns.catplot(x=col, y="Price_euros", kind="bar", dodge=False, height = 4, aspect = 3,  data=df, palette="Set3")
  plt.title("Rata-rata 'price' Relatif terhadap - {}".format(col))

"""## **Terdapat beberapa poin yang didapat berdasarkan catplot diatas**

* Ternyata merk cukup berpengaruh terhadap harga, dapat dilihat merk chuwi memiliki harga laptop paling murah sedangkan merk laptop LG memiliki harga tingi
* Sistem operasi laptop juga ternyata cukup berpengaruh terhadap harga, dapat dilihat laptop dengan sistem operasi macOS memiliki harga paling tinggi
* Tipe laptop juga memiliki pengaruh terhadap harga, tipe notebook memiliki harga paling murah sedangkan yang paling mahal adalah workstation
* Seri CPU juga memiliki pengaruh terhadap harga, laptop dengan i7 cenderung memiliki harga lebih mahal dan seri M memiliki harga tertinggi
* Jenis memory/storage juga memiliki pengaruh terhadap harga, dimana laptop dengan harga SSD memiliki harga paling mahal disusul dengan jenis Hybrid
* Karena memory_2_type hanya memiliki 1 data saja yaitu None, maka kolom tersebut dihapus
"""

df.drop(['memory_2_type'], axis=1,inplace=True)
cat_columns.remove('memory_2_type')

sns.pairplot(df, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr().round(2)
 
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""* Besar RAM memiliki pengaruh besar terhadap harga laptop
* Karena kolom Inches, weight_float, dan memory_1_size memiliki korelasi yang kecil terhadap price maka kolom tersebut didrop




"""

# drop Inches, weight_float, memory_1_size, memory_count, dan memory_2_size karena korelasi yang kecil dengan price
df.drop(['Inches', 'weight_float', 'memory_1_size'], inplace=True, axis=1)
df.head()

"""#**Data Preparation**

Melakukan Encoding Fitur Kategori
"""

for col in cat_columns:
  df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)
  df.drop([col], axis=1, inplace=True)
df.head()

"""Melakukan Train-Test-Split"""

y = df["Price_euros"]
X = df.drop(["Price_euros"],axis =1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 55)

"""Melakukan Standarisasi Data"""

scaler = StandardScaler()
numerical_features = X_train.select_dtypes(include=['float','int']).columns.to_list()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

"""Mengecek apakah X_train sudah terstandarisasi"""

X_train[numerical_features].describe().round(4)

"""# **Modelling**"""

models = pd.DataFrame(index=['train_mse', 'test_mse'], 
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""Model Development KNN"""

knn = KNeighborsRegressor(algorithm='kd_tree', n_neighbors=10)
knn.fit(X_train, y_train)
 
models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""Model Development Random Forest"""

RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)
 
models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""Model Development Boosting"""

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)                             
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""Standarisasi data untuk data test"""

X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

"""# **Evaluasi Model**"""

mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])
 
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}

for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3 
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3
 
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

prediksi = X_test.iloc[:-1].copy()
pred_dict = {'y_true':y_test[:-1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)

"""Berdasar hasil prediksi model saat ini yang paling baik adalah random forest karena memiliki MSE terkecil pada data train dan test

# **Hyperparameter Tuning**

Hyperparameter Tuning untuk KNN dengan GridSearch
"""

parameters = {
    'n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30],
    'algorithm' : ["auto",  "ball_tree", "kd_tree", "brute"],
}

clf = GridSearchCV(estimator = knn, param_grid = parameters, 
                          cv = 5)
clf.fit(X_train, y_train)

print(clf.best_estimator_)
print('n_neighbors : ',(clf.best_estimator_.n_neighbors))

knn_base_mse = evaluate(knn, X_test, y_test)

knn_best_grid = clf.best_estimator_
knn_best_grid.fit(X_train, y_train)

knn_grid_mse = evaluate(knn_best_grid, X_test, y_test)
print('Improvement of {:0.2f}%.'.format( 100 * (knn_base_mse - knn_grid_mse) / knn_grid_mse))

knn = knn_best_grid
knn.fit(X_train, y_train)
 
models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""Hyperparameter tuning untuk Random Forest"""

param_grid = {
    'bootstrap': [True, False],
    'max_depth': [8, 16, 32, 64],
    'max_features': ['auto', 'sqrt'],
    'min_samples_leaf': [1, 2, 4],
    'min_samples_split': [2, 5, 10],
    'n_estimators': [60, 70, 80, 90, 100],
    'random_state': [55]
}
rf_grid_search = GridSearchCV(estimator = RF, param_grid = param_grid, 
                          cv = 5)

rf_base_model = RF
rf_base_model.fit(X_train, y_train)
rf_base_mse = evaluate(rf_base_model, X_test, y_test)

rf_grid_search.fit(X_train, y_train)
rf_grid_search.best_params_

rf_best_grid = rf_grid_search.best_estimator_
rf_grid_mse = evaluate(rf_best_grid, X_test, y_test)
print('Improvement of {:0.2f}%.'.format( 100 * (rf_base_mse - rf_grid_mse) / rf_grid_mse))

RF = rf_best_grid
RF.fit(X_train, y_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""Adaboost hyperparameter tuning"""

ada = AdaBoostRegressor()
search_grid={
    'n_estimators':[500,1000,1500],
    'learning_rate':[0.001,0.005, 0.01, 0.05, 0.1],
    'random_state':[55]}
boosting_grid=GridSearchCV(estimator=ada,
                    param_grid=search_grid,
                    cv=5)

boosting_grid.fit(X_train,y_train)
boosting_grid.best_params_

boost_base_mse = evaluate(boosting, X_test, y_test)

boost_grid_mse = evaluate(boosting_grid, X_test, y_test)
print('Improvement of {:0.2f}%.'.format( 100 * (boost_base_mse - boost_grid_mse) / boost_grid_mse))

boosting = boosting_grid                           

models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""# **Evaluasi Model setelah hyperparameter tuning**"""

model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}
 
# # Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3 
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3
 
# Panggil mse
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

prediksi = X_test.iloc[:-1].copy()
pred_dict = {'y_true':y_test[:-1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)

"""Dari ketiga algoritma tersebut dapat disimpulkan yang terbaik adalah random forest karena memiliki MSE pada train dan test terkecil. Dan juga ketiga algoritma tersebut setelah dilakukan hyperparameter tuning mengalami peningkatan dan peningkatan terbaik diraih oleh random forest."""